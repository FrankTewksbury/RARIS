---
description: Global Implementation Standards for Gemini 3.1, Agentic Workflows, and Resilient RAG
alwaysApply: false
---
# Gemini 3.1 Global Implementation Rules

> **MANDATORY:** Do not rely on pre-2025 training data regarding model IDs or SDK patterns. Always use the implementation patterns defined in this document for all Python-based Gemini integrations.

---

## 1. Model Selection & Lifecycle

| Model ID | Primary Use Case | Thinking Budget | Notes |
|----------|-----------------|-----------------|-------|
| `gemini-3.1-pro-preview` | **DEFAULT** - Complex reasoning, multi-step discovery | 32768 | Replaces all 3.0/1.5 Pro models |
| `gemini-3.1-pro-preview-customtools` | **AGENTIC** - Bash, filesystem, local tool execution | 24576 | Specialized for tool-calling accuracy |
| `gemini-3-flash-preview` | High-speed batching, entity extraction, classification | 4096 | Gemini 3 Flash (no 3.1 Flash exists) |
| `gemini-3.1-flash-image-preview` | Vision, OCR, document layout analysis | 4096 | Powered by Nano Banana 2 |

### Deprecated Models (DO NOT USE)

* `gemini-3-pro-preview` (Retires March 9, 2026)
* `gemini-1.5-pro` / `gemini-1.5-flash`
* `gemini-pro` / `gemini-ultra`

---

## 2. Core API Implementation (Python SDK)

### 2.1 SDK Initialization

Use the `google-genai` modern SDK. Do not use legacy `google-generativeai`.

```python
from google import genai
from google.genai import types
import os

client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
```

### 2.2 Standard Generation Pattern

Prefer `generate_content` over legacy "completion" terminology. Use `ThinkingConfig` for all reasoning tasks.

```python
def call_gemini(prompt: str, model: str = "gemini-3.1-pro-preview", budget: int = 16384):
    config = types.GenerateContentConfig(
        temperature=0.7,
        thinking_config=types.ThinkingConfig(thinking_budget=budget)
    )

    response = client.models.generate_content(
        model=model,
        contents=prompt,
        config=config
    )
    return response.text
```

---

## 3. Web Grounding with Google Search

Enable the built-in `google_search` tool to ground LLM responses in live web results. This eliminates hallucinated URLs and provides verifiable citations.

### 3.1 Basic Grounded Generation

```python
from google import genai
from google.genai import types

client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

grounding_tool = types.Tool(google_search=types.GoogleSearch())
config = types.GenerateContentConfig(
    tools=[grounding_tool],
    thinking_config=types.ThinkingConfig(thinking_budget=16384)
)

response = client.models.generate_content(
    model="gemini-3.1-pro-preview",
    contents="Find current down payment assistance programs in Texas",
    config=config,
)
print(response.text)
```

### 3.2 Accessing Grounding Metadata

When grounding is active, the response includes `grounding_metadata` with search queries, sources, and citations.

```python
candidate = response.candidates[0]
if candidate.grounding_metadata:
    metadata = candidate.grounding_metadata
    if metadata.grounding_chunks:
        for chunk in metadata.grounding_chunks:
            if chunk.web:
                print(f"  {chunk.web.title}: {chunk.web.uri}")
```

### 3.3 When to Use Grounding

| Use Case | Grounding? | Rationale |
|----------|-----------|-----------|
| Discovery — finding entities, URLs, programs | **YES** | Prevents hallucinated URLs, gets current data |
| Verification — confirming a URL or program exists | **YES** | Live web check vs. training data guess |
| Classification — categorizing already-retrieved text | No | Input is already grounded, no web needed |
| JSON extraction — parsing structured data from text | No | Deterministic task, search adds latency |

### 3.4 Grounding + Thinking Interaction

Grounding and thinking work together in 3.1. The model uses its thinking budget to reason about *when* to search and *how* to synthesize results. Both can be enabled simultaneously in the same `GenerateContentConfig`.

---

## 4. Advanced Functionality

### 4.1 PDF & Large Document Discovery (Files API)

For all documents > 1MB, use the Files API. Native text extraction is non-billable.

```python
def upload_and_analyze_document(file_path: str, instruction: str):
    uploaded_file = client.files.upload(path=file_path)

    response = client.models.generate_content(
        model="gemini-3.1-pro-preview",
        contents=[uploaded_file, instruction]
    )
    return response.text
```

### 4.2 Session Resumption (Multi-Turn Persistence)

Required for long-horizon tasks where the model must maintain context for up to 24 hours despite network interruptions.

```python
chat_session = client.chats.create(
    model="gemini-3.1-pro-preview",
    config=types.GenerateContentConfig(session_resumption=True)
)
```

---

## 5. Resilience & Error Handling

### 5.1 Deterministic Fallback Chain

```
gemini-3.1-pro-preview  →  gemini-3.1-pro-preview:no-think  →  gemini-3-flash-preview
```

The `:no-think` suffix is a convention handled in application code — strip the suffix and set `thinking_budget=0` before calling the API.

### 5.2 Retry Policy

| HTTP Code | Action |
|-----------|--------|
| 429, 503, 504 | Retry with backoff **and** downgrade to next model in chain |
| 500, 502 | Retry with backoff, same model |
| 400, 401, 403, 404 | **Fail fast** — do not retry (request shape or auth issue) |

### 5.3 Resilient Async Pattern

```python
import asyncio
from google.genai import errors

RETRYABLE = {429, 500, 502, 503, 504}
DOWNGRADE = {429, 503, 504}

FALLBACK_CHAIN = [
    "gemini-3.1-pro-preview",
    "gemini-3.1-pro-preview:no-think",
    "gemini-3-flash-preview",
]

async def resilient_call(contents, config, chain=FALLBACK_CHAIN):
    idx = 0
    for attempt in range(4):
        entry = chain[min(idx, len(chain) - 1)]
        if entry.endswith(":no-think"):
            current_model = entry.removesuffix(":no-think")
            attempt_config = types.GenerateContentConfig(
                thinking_config=types.ThinkingConfig(thinking_budget=0)
            )
        else:
            current_model = entry
            attempt_config = config
        try:
            return await client.aio.models.generate_content(
                model=current_model,
                contents=contents,
                config=attempt_config,
            )
        except errors.APIError as e:
            if e.code not in RETRYABLE:
                raise
            if e.code in DOWNGRADE:
                idx += 1
            await asyncio.sleep(min(2 ** attempt + random.uniform(0, 1), 32))
```

---

## 6. Structured Data & Embeddings

### 6.1 JSON Mode Enforcement

```python
config = types.GenerateContentConfig(
    response_mime_type="application/json",
    response_schema={
        "type": "object",
        "properties": {
            "analysis": {"type": "string"},
            "entities": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["analysis"]
    }
)
```

### 6.2 Embeddings

Use `text-embedding-005` for all vector database integrations.

```python
def get_vector(text: str):
    return client.models.embed_content(
        model="text-embedding-005",
        content=text
    ).embedding
```

---

## 7. Quick Reference

### Environment Variables

```
GEMINI_API_KEY=your_api_key_here
GEMINI_MODEL=gemini-3.1-pro-preview
GEMINI_FALLBACK_MODELS=gemini-3.1-pro-preview,gemini-3.1-pro-preview:no-think,gemini-3-flash-preview
GEMINI_THINKING_BUDGET=16384
```
